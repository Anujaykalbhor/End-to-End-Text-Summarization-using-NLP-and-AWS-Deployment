{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9311cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os        #  Actually I process these notebook on Lightning AI platform, and later save that file inside here why because taking to long on my system without GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a382138-55b0-4dee-a9ea-41a9038e270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment'\n",
      "/teamspace/studios/this_studio/End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment/research\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "cd End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d2850d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment/research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a640fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0f6fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ada959-2be8-4505-ba7d-13dbae9946e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added '/teamspace/studios/this_studio/End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment/src' to Python Path.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# # Get the absolute path to the current directory of the notebook\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# # Find the project root by going up until we find the 'src' folder\n",
    "# project_root = current_dir\n",
    "# while 'src' not in os.listdir(project_root):\n",
    "#     parent_dir = os.path.dirname(project_root)\n",
    "#     if parent_dir == project_root: # Reached the file system root\n",
    "#         break\n",
    "#     project_root = parent_dir\n",
    "\n",
    "# # Add the project's 'src' directory to the Python path\n",
    "# src_path = os.path.join(project_root, 'src')\n",
    "# if src_path not in sys.path:\n",
    "#     sys.path.append(src_path)\n",
    "#     print(f\"Added '{src_path}' to Python Path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961c7797-7365-4f1f-8a29-0a310385af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from dataclasses import dataclass\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "# @dataclass(frozen=True)\n",
    "# class ModelTrainerConfig:\n",
    "#     root_dir: Path\n",
    "#     data_path: Path\n",
    "#     model_ckpt: Path\n",
    "#     num_train_epochs: int\n",
    "#     warmup_steps: int\n",
    "#     per_device_train_batch_size: int\n",
    "#     weight_decay: float\n",
    "#     logging_steps: int\n",
    "#     evaluation_strategy: str\n",
    "#     eval_steps: int\n",
    "#     save_steps: float\n",
    "#     gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd1ad67-b759-4f1a-a34f-63f82e464c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text_summerization_project_nlp.constants import *\n",
    "# from text_summerization_project_nlp.utils.common import read_yaml, create_directories\n",
    "# from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090555a0-ecb1-4c90-843f-2a3c4f733939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pathlib import Path\n",
    "# from text_summerization_project_nlp.constants import *\n",
    "# from text_summerization_project_nlp.utils.common import read_yaml, create_directories\n",
    "\n",
    "# class ConfigurationManager:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         config_filepath = Path(\"config/config.yaml\"),\n",
    "#         params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "#         self.config = read_yaml(config_filepath)\n",
    "#         self.params = read_yaml(params_filepath)\n",
    "\n",
    "#         create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "#     def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "#         config = self.config.model_trainer\n",
    "#         params = self.params.TrainingArguments\n",
    "\n",
    "#         create_directories([config.root_dir])\n",
    "\n",
    "#         model_trainer_config = ModelTrainerConfig(\n",
    "#             root_dir=config.root_dir,\n",
    "#             data_path=config.data_path,\n",
    "#             model_ckpt = config.model_ckpt,\n",
    "#             num_train_epochs = params.num_train_epochs,\n",
    "#             warmup_steps = params.warmup_steps,\n",
    "#             per_device_train_batch_size = params.per_device_train_batch_size,\n",
    "#             weight_decay = params.weight_decay,\n",
    "#             logging_steps = params.logging_steps,\n",
    "#             evaluation_strategy = params.evaluation_strategy,\n",
    "#             eval_steps = params.eval_steps,  # <-- This should be params.eval_steps\n",
    "#             save_steps = params.save_steps,\n",
    "#             gradient_accumulation_steps = params.gradient_accumulation_steps\n",
    "#         )\n",
    "\n",
    "#         return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86ac4d18-fe4a-481d-8d5c-147d9ff4c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "# from transformers import DataCollatorForSeq2Seq\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# from datasets import load_dataset, load_from_disk\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea1ec12b-6946-4a09-8640-4591d59b7265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added '/teamspace/studios/this_studio/End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment/src' to Python Path.\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1881/2022269537.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2305' max='2305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2305/2305 21:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.579200</td>\n",
       "      <td>1.451499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.478900</td>\n",
       "      <td>1.382670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.406300</td>\n",
       "      <td>1.359203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.348200</td>\n",
       "      <td>1.351458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py:4034: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to artifacts/model_trainer\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import torch\n",
    "# from datasets import load_from_disk\n",
    "# from transformers import AutoModelForSeq2SeqLM, PegasusTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "# from pathlib import Path\n",
    "# from box.exceptions import BoxValueError\n",
    "# import yaml\n",
    "# import logging\n",
    "\n",
    "\n",
    "# # --- Fix for ModuleNotFoundError ---\n",
    "# # Get the absolute path to the current directory of the notebook\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# # Find the project root by going up until we find the 'src' folder\n",
    "# project_root = current_dir\n",
    "# while 'src' not in os.listdir(project_root):\n",
    "#     parent_dir = os.path.dirname(project_root)\n",
    "#     if parent_dir == project_root:\n",
    "#         break\n",
    "#     project_root = parent_dir\n",
    "\n",
    "# # Add the project's 'src' directory to the Python path\n",
    "# src_path = os.path.join(project_root, 'src')\n",
    "# if src_path not in sys.path:\n",
    "#     sys.path.append(src_path)\n",
    "\n",
    "# print(f\"Added '{src_path}' to Python Path.\")\n",
    "\n",
    "# # --- The actual code ---\n",
    "# # A simplified version of your ConfigurationManager and logging\n",
    "# def read_yaml(path_to_yaml: Path):\n",
    "#     try:\n",
    "#         with open(path_to_yaml) as yaml_file:\n",
    "#             content = yaml.safe_load(yaml_file)\n",
    "#             return content\n",
    "#     except BoxValueError:\n",
    "#         raise ValueError(\"yaml file is empty\")\n",
    "#     except Exception as e:\n",
    "#         raise e\n",
    "\n",
    "# def create_directories(path_to_directories: list):\n",
    "#     for path in path_to_directories:\n",
    "#         os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# # Define a simple configuration class based on your provided yaml content\n",
    "# class ModelTrainerConfig:\n",
    "#     def __init__(self, root_dir, data_path, model_ckpt):\n",
    "#         self.root_dir = Path(root_dir)\n",
    "#         self.data_path = Path(data_path)\n",
    "#         self.model_ckpt = model_ckpt\n",
    "#         create_directories([self.root_dir])\n",
    "\n",
    "# # The ModelTrainer class\n",
    "# class ModelTrainer:\n",
    "#     def __init__(self, config: ModelTrainerConfig):\n",
    "#         self.config = config\n",
    "\n",
    "#     def train(self):\n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         print(f\"Using device: {device}\")\n",
    "\n",
    "#         # Corrected: Explicitly import and use PegasusTokenizer\n",
    "#         tokenizer = PegasusTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "#         model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "\n",
    "#         seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "\n",
    "#         # Loading data\n",
    "#         dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "#         trainer_args = TrainingArguments(\n",
    "#             output_dir=self.config.root_dir,\n",
    "#             num_train_epochs=5,\n",
    "#             warmup_steps=500,\n",
    "#             per_device_train_batch_size=8,  # Increased effective batch size\n",
    "#             per_device_eval_batch_size=1,\n",
    "#             weight_decay=0.01,\n",
    "#             logging_steps=10,\n",
    "#             eval_strategy='steps',\n",
    "#             eval_steps=500,\n",
    "#             save_steps=1e6,\n",
    "#             gradient_accumulation_steps=4,  # Adjusted to maintain effective batch size\n",
    "#             fp16=True  # Added for mixed precision training\n",
    "#         )\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model=model_pegasus,\n",
    "#             args=trainer_args,\n",
    "#             tokenizer=tokenizer,\n",
    "#             data_collator=seq2seq_data_collator,\n",
    "#             train_dataset=dataset_samsum_pt[\"train\"],\n",
    "#             eval_dataset=dataset_samsum_pt[\"validation\"]\n",
    "#         )\n",
    "\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Save model and tokenizer to the same directory\n",
    "#         model_pegasus.save_pretrained(self.config.root_dir)\n",
    "#         tokenizer.save_pretrained(self.config.root_dir)\n",
    "#         print(f\"Model and tokenizer saved to {self.config.root_dir}\")\n",
    "\n",
    "# # --- Main execution block ---\n",
    "# if __name__ == '__main__':\n",
    "#     # You will need to define your config data here or load it from config.yaml\n",
    "#     config_data = {\n",
    "#         \"model_trainer\": {\n",
    "#             \"root_dir\": \"artifacts/model_trainer\",\n",
    "#             \"data_path\": \"artifacts/data_transformation/samsum_dataset\",\n",
    "#             \"model_ckpt\": \"google/pegasus-cnn_dailymail\"\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     model_trainer_config_data = config_data['model_trainer']\n",
    "\n",
    "#     model_trainer_config = ModelTrainerConfig(\n",
    "#         root_dir=model_trainer_config_data['root_dir'],\n",
    "#         data_path=model_trainer_config_data['data_path'],\n",
    "#         model_ckpt=model_trainer_config_data['model_ckpt']\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "#         model_trainer.train()\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee34262-5e3d-410e-b21e-1cbfd0a44713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "829e9f3a-6260-4814-af9e-16c11f2ee455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'knkarthick/samsum' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-05 14:18:57,416: ERROR:load: `trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'knkarthick/samsum' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770b0dbd33654acb89b38385b1a1170a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1881/3216648017.py:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11049' max='11049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11049/11049 36:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import py7zr # Added to handle the dataset's compression format\n",
    "\n",
    "# Acknowledge the use of a GPU for faster training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Load the FLAN-T5 model and tokenizer ---\n",
    "# Define the pre-trained model name\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# --- 2. Load and preprocess the SAMSum dataset ---\n",
    "# The original 'samsum' dataset is no longer directly available.\n",
    "# We will use the community-maintained version 'knkarthick/samsum'.\n",
    "# The `trust_remote_code=True` argument is still required for this dataset.\n",
    "dataset = load_dataset(\"knkarthick/samsum\", trust_remote_code=True)\n",
    "\n",
    "# Load the ROUGE metric for evaluation\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # The `flan-t5` model requires a specific format with a task prefix\n",
    "    inputs = [f\"summarize: {dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "    \n",
    "    # Tokenize the inputs and pad them\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=True)\n",
    "    \n",
    "    # Tokenize the labels (summaries) separately\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True, padding=True)\n",
    "    \n",
    "    # Assign the tokenized labels to the model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the entire dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "\n",
    "# --- 3. Set up the Training and Evaluation Configuration ---\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=\"longest\")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, # Enable mixed precision training for efficiency\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\" # Disable reporting to external services\n",
    ")\n",
    "\n",
    "# --- 4. Define the evaluation function ---\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode the generated predictions and reference labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute the ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Add a friendly output format\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    \n",
    "    return result\n",
    "\n",
    "# --- 5. Initialize and run the Trainer ---\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training!\n",
    "trainer.train()\n",
    "\n",
    "# --- 6. Save the fine-tuned model ---\n",
    "trainer.save_model(\"./fine-tuned-flan-t5-samsum\")\n",
    "\n",
    "print(\"Fine-tuning complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a8e7989-e752-4718-9f8e-17796faaefa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--- Starting Training Loop Debug ---\n",
      "\n",
      "-- Batch 0 Inspection --\n",
      "Inputs shape: torch.Size([32, 10])\n",
      "Labels shape: torch.Size([32, 1])\n",
      "Sample inputs (first 5): \n",
      "[[ 0.5901335   1.7704183  -0.07067952  0.4712788  -1.0674683  -1.4308237\n",
      "   1.5921277   2.4104173  -0.13301812 -0.20999406]\n",
      " [-1.0221963   0.57261455 -0.0077945   1.6709948  -0.41744235  0.36366972\n",
      "  -0.5823153   0.8814537  -0.25979808 -0.51184934]\n",
      " [ 0.7260908   0.51979715  0.27226296  0.8830055   0.84551865  0.11310846\n",
      "   0.3692969  -0.37612525 -0.17815088  0.05414319]\n",
      " [ 0.180822    0.20266859 -1.2098444  -1.3993149  -0.38525784  0.45761526\n",
      "   0.33387703  1.2617142  -0.46432668  0.19130388]\n",
      " [-0.2977356   1.3464781   0.82898724  0.3345687   0.02919182  0.1296838\n",
      "   1.0744873  -2.0292013  -0.34864447  0.3044129 ]]\n",
      "Sample labels (first 5): \n",
      "[[7.5260596 ]\n",
      " [0.54715836]\n",
      " [4.0189123 ]\n",
      " [2.0377166 ]\n",
      " [4.4895434 ]]\n",
      "------------------------\n",
      "Epoch [1/5], Batch [1/32], Raw Loss: 17.3691673279\n",
      "\n",
      "-- Gradient Inspection (after backward) --\n",
      "Gradient for 'layer1.weight' is non-zero? True\n",
      "Gradient for 'layer1.bias' is non-zero? True\n",
      "Gradient for 'layer2.weight' is non-zero? True\n",
      "Gradient for 'layer2.bias' is non-zero? True\n",
      "Gradient for 'output_layer.weight' is non-zero? True\n",
      "Gradient for 'output_layer.bias' is non-zero? True\n",
      "----------------------------------------\n",
      "Epoch [1/5], Batch [11/32], Raw Loss: 4.7063608170\n",
      "Epoch [1/5], Batch [21/32], Raw Loss: 2.9884221554\n",
      "Epoch [1/5], Batch [31/32], Raw Loss: 0.8971567154\n",
      "Epoch 1 finished. Final loss for epoch: 0.2357\n",
      "\n",
      "-- Batch 0 Inspection --\n",
      "Inputs shape: torch.Size([32, 10])\n",
      "Labels shape: torch.Size([32, 1])\n",
      "Sample inputs (first 5): \n",
      "[[-1.588916    0.73853976  0.7582192   1.6741248   0.54419136  2.3304772\n",
      "   0.02989144  1.4284664   3.062358   -0.3296282 ]\n",
      " [ 1.8028808   0.59557736 -0.27263236  0.91514975  1.220679   -0.675113\n",
      "   1.201409   -0.48899963  0.12248944 -1.2153677 ]\n",
      " [ 0.168953    0.14423542 -0.21257566 -0.52476084 -0.85219765 -0.80340636\n",
      "  -0.28484222  1.0433104   1.9658667   0.12835433]\n",
      " [-0.8988501  -1.329926   -0.46640146  0.6533105   1.5200921   0.7787331\n",
      "  -0.5344379   0.1178103   0.6112923  -0.8917303 ]\n",
      " [-1.8958459  -1.5788594   0.16241594 -0.31313047 -0.20832202 -1.1174812\n",
      "   0.06889024 -0.28107515  0.5194681   0.4604727 ]]\n",
      "Sample labels (first 5): \n",
      "[[ 0.08406235]\n",
      " [ 6.4569006 ]\n",
      " [ 1.8668542 ]\n",
      " [-4.6707773 ]\n",
      " [-7.5662065 ]]\n",
      "------------------------\n",
      "Epoch [2/5], Batch [1/32], Raw Loss: 0.5799182057\n",
      "\n",
      "-- Gradient Inspection (after backward) --\n",
      "Gradient for 'layer1.weight' is non-zero? True\n",
      "Gradient for 'layer1.bias' is non-zero? True\n",
      "Gradient for 'layer2.weight' is non-zero? True\n",
      "Gradient for 'layer2.bias' is non-zero? True\n",
      "Gradient for 'output_layer.weight' is non-zero? True\n",
      "Gradient for 'output_layer.bias' is non-zero? True\n",
      "----------------------------------------\n",
      "Epoch [2/5], Batch [11/32], Raw Loss: 0.4268499315\n",
      "Epoch [2/5], Batch [21/32], Raw Loss: 0.2320410460\n",
      "Epoch [2/5], Batch [31/32], Raw Loss: 0.0718782842\n",
      "Epoch 2 finished. Final loss for epoch: 0.0421\n",
      "\n",
      "-- Batch 0 Inspection --\n",
      "Inputs shape: torch.Size([32, 10])\n",
      "Labels shape: torch.Size([32, 1])\n",
      "Sample inputs (first 5): \n",
      "[[ 0.28677016  0.7208169  -1.71282     1.4218016   0.32742184  1.3732454\n",
      "   2.3293285   1.6961102   0.8616948   0.36282572]\n",
      " [-0.21722436  2.2732248  -0.4477569   0.3243776  -0.2037183   2.3764145\n",
      "   1.4627321   0.4399219  -0.02146243 -0.55687076]\n",
      " [-0.3951223  -0.90298015 -1.2786834   0.07468026  0.99236387  1.4007512\n",
      "  -0.7555637  -0.3591204   0.4049714   1.6793983 ]\n",
      " [-0.8504212   0.19107656 -0.6336938  -0.5695517  -0.7449278  -2.3360736\n",
      "   0.8398008  -0.25335845 -0.23675817  0.25613585]\n",
      " [-0.57815367 -0.08863566 -1.1321138  -0.12482402  0.09003339 -0.7350587\n",
      "  -0.18533148 -0.43712223  0.14888462 -2.161773  ]]\n",
      "Sample labels (first 5): \n",
      "[[ 3.6271133 ]\n",
      " [ 7.381537  ]\n",
      " [-2.5840533 ]\n",
      " [-0.12241076]\n",
      " [-0.41909575]]\n",
      "------------------------\n",
      "Epoch [3/5], Batch [1/32], Raw Loss: 0.0684712380\n",
      "\n",
      "-- Gradient Inspection (after backward) --\n",
      "Gradient for 'layer1.weight' is non-zero? True\n",
      "Gradient for 'layer1.bias' is non-zero? True\n",
      "Gradient for 'layer2.weight' is non-zero? True\n",
      "Gradient for 'layer2.bias' is non-zero? True\n",
      "Gradient for 'output_layer.weight' is non-zero? True\n",
      "Gradient for 'output_layer.bias' is non-zero? True\n",
      "----------------------------------------\n",
      "Epoch [3/5], Batch [11/32], Raw Loss: 0.0490116104\n",
      "Epoch [3/5], Batch [21/32], Raw Loss: 0.0897538140\n",
      "Epoch [3/5], Batch [31/32], Raw Loss: 0.0475805402\n",
      "Epoch 3 finished. Final loss for epoch: 0.0285\n",
      "\n",
      "-- Batch 0 Inspection --\n",
      "Inputs shape: torch.Size([32, 10])\n",
      "Labels shape: torch.Size([32, 1])\n",
      "Sample inputs (first 5): \n",
      "[[-0.57206345  0.45152476  0.3498132   1.1253419  -0.19112672 -0.3112581\n",
      "   0.7077068  -1.4343137   1.4623061  -0.0433947 ]\n",
      " [ 0.57355165 -1.1903584   0.16734628 -2.1950614  -0.32618433 -0.01054026\n",
      "  -0.01373762  0.3387031   1.4267993   0.10572939]\n",
      " [-2.196825    0.16219592 -0.07735085  0.4990763   0.60300833 -0.3283985\n",
      "   2.2648208  -1.5099273   2.4116845  -2.4410574 ]\n",
      " [-3.1561139  -1.3029263  -0.08322698 -0.3564068   0.20808797  0.5364522\n",
      "   0.7536429   2.1165414  -1.0280666   0.5579478 ]\n",
      " [-0.30741912  0.95291203  2.621592    0.0468295   1.8982393   0.2102842\n",
      "  -0.19714563  1.1628803  -1.5484445  -0.7528288 ]]\n",
      "Sample labels (first 5): \n",
      "[[ 1.135123 ]\n",
      " [-1.4813725]\n",
      " [-3.009567 ]\n",
      " [-9.103399 ]\n",
      " [ 3.2996347]]\n",
      "------------------------\n",
      "Epoch [4/5], Batch [1/32], Raw Loss: 0.0239495412\n",
      "\n",
      "-- Gradient Inspection (after backward) --\n",
      "Gradient for 'layer1.weight' is non-zero? True\n",
      "Gradient for 'layer1.bias' is non-zero? True\n",
      "Gradient for 'layer2.weight' is non-zero? True\n",
      "Gradient for 'layer2.bias' is non-zero? True\n",
      "Gradient for 'output_layer.weight' is non-zero? True\n",
      "Gradient for 'output_layer.bias' is non-zero? True\n",
      "----------------------------------------\n",
      "Epoch [4/5], Batch [11/32], Raw Loss: 0.0420102850\n",
      "Epoch [4/5], Batch [21/32], Raw Loss: 0.0547700115\n",
      "Epoch [4/5], Batch [31/32], Raw Loss: 0.0408989117\n",
      "Epoch 4 finished. Final loss for epoch: 0.0317\n",
      "\n",
      "-- Batch 0 Inspection --\n",
      "Inputs shape: torch.Size([32, 10])\n",
      "Labels shape: torch.Size([32, 1])\n",
      "Sample inputs (first 5): \n",
      "[[-0.31037295  0.77646106  0.12773779  1.6040264  -0.1710022  -0.3709566\n",
      "  -1.1569059  -0.12876745 -1.8543056  -0.3319994 ]\n",
      " [ 0.6436244  -1.0684239  -1.4743813   0.03841957 -0.63679     1.4328343\n",
      "  -2.0496752  -0.89094704  0.5285426  -2.0032823 ]\n",
      " [ 1.1512747  -0.5254521   0.55079764  0.32177353 -0.78328466  0.69978994\n",
      "  -0.78591686 -1.0037284  -1.0495561  -1.5698181 ]\n",
      " [ 0.14877082  2.5665667   0.52502763 -0.34721342 -0.54634947 -0.20155612\n",
      "  -1.6750215  -1.4725717  -0.58118266  0.36472413]\n",
      " [ 0.7700077   0.36839947 -0.7939407  -2.968407    0.26382664 -0.64567864\n",
      "  -1.5920966  -0.5059308   0.09594703 -0.6593146 ]]\n",
      "Sample labels (first 5): \n",
      "[[ 2.4487672]\n",
      " [-1.064697 ]\n",
      " [ 1.7341702]\n",
      " [ 8.968858 ]\n",
      " [ 3.6735163]]\n",
      "------------------------\n",
      "Epoch [5/5], Batch [1/32], Raw Loss: 0.0265758429\n",
      "\n",
      "-- Gradient Inspection (after backward) --\n",
      "Gradient for 'layer1.weight' is non-zero? True\n",
      "Gradient for 'layer1.bias' is non-zero? True\n",
      "Gradient for 'layer2.weight' is non-zero? True\n",
      "Gradient for 'layer2.bias' is non-zero? True\n",
      "Gradient for 'output_layer.weight' is non-zero? True\n",
      "Gradient for 'output_layer.bias' is non-zero? True\n",
      "----------------------------------------\n",
      "Epoch [5/5], Batch [11/32], Raw Loss: 0.0315236971\n",
      "Epoch [5/5], Batch [21/32], Raw Loss: 0.0247744564\n",
      "Epoch [5/5], Batch [31/32], Raw Loss: 0.0398836136\n",
      "Epoch 5 finished. Final loss for epoch: 0.0496\n",
      "\n",
      "--- Training finished ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 1. Simulate a simple dataset\n",
    "class DummyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dummy dataset for a simple regression problem.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples=1000):\n",
    "        # Create random input features\n",
    "        self.X = torch.randn(num_samples, 10)\n",
    "        # Create corresponding labels (a simple linear relationship with some noise)\n",
    "        self.y = 2 * self.X[:, 0] + 3 * self.X[:, 1] + 1 + 0.1 * torch.randn(num_samples)\n",
    "        # Reshape y to have a shape of [num_samples, 1] for the model's output\n",
    "        self.y = self.y.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 2. Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic neural network with a few linear layers.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# 3. Instantiate model, data, loss, and optimizer\n",
    "dataset = DummyDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "model = SimpleModel()\n",
    "criterion = nn.MSELoss() # Mean Squared Error for a regression task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"--- Starting Training Loop Debug ---\")\n",
    "\n",
    "# 4. Training loop with debugging steps\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        # Move data to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # DEBUGGING STEP 1: Inspect the input data and labels\n",
    "        # A loss of 0 could mean the data is all zeros or the labels are all the same.\n",
    "        if i == 0:\n",
    "            print(\"\\n-- Batch 0 Inspection --\")\n",
    "            print(f\"Inputs shape: {inputs.shape}\")\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "            print(f\"Sample inputs (first 5): \\n{inputs[:5, :].detach().cpu().numpy()}\")\n",
    "            print(f\"Sample labels (first 5): \\n{labels[:5].detach().cpu().numpy()}\")\n",
    "            print(\"------------------------\")\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # DEBUGGING STEP 2: Print the full loss value\n",
    "        # Sometimes the loss is very small but not exactly zero.\n",
    "        # This print statement shows the full precision.\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], Raw Loss: {loss.item():.10f}\")\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # DEBUGGING STEP 3: Check gradients after backward pass\n",
    "        # If gradients are all zero, parameters won't be updated.\n",
    "        if i == 0:\n",
    "            print(\"\\n-- Gradient Inspection (after backward) --\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    # You can also use param.grad.norm() to check the magnitude\n",
    "                    print(f\"Gradient for '{name}' is non-zero? {torch.any(param.grad != 0)}\")\n",
    "            print(\"----------------------------------------\")\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished. Final loss for epoch: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n--- Training finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314f721-df89-448b-bf01-0c69e51f70b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

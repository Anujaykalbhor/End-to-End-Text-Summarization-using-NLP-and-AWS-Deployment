{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bbd1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b1d9b55-17bd-4a58-ae0b-6c6cba353a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment'\n",
      "/teamspace/studios/this_studio/End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment/research\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "cd End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15f4fc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af12b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68cbac85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50b15f31-498c-4a1e-9874-c815f6a59249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# # Get the absolute path to the current directory of the notebook\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# # Find the project root by going up until we find the 'src' folder\n",
    "# project_root = current_dir\n",
    "# while 'src' not in os.listdir(project_root):\n",
    "#     parent_dir = os.path.dirname(project_root)\n",
    "#     if parent_dir == project_root: # Reached the file system root\n",
    "#         break\n",
    "#     project_root = parent_dir\n",
    "\n",
    "# # Add the project's 'src' directory to the Python path\n",
    "# src_path = os.path.join(project_root, 'src')\n",
    "# if src_path not in sys.path:\n",
    "#     sys.path.append(src_path)\n",
    "#     print(f\"Added '{src_path}' to Python Path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf28d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "# @dataclass(frozen=True)\n",
    "# class ModelEvaluationConfig:\n",
    "#     root_dir: Path\n",
    "#     data_path: Path\n",
    "#     model_path: Path\n",
    "#     tokenizer_path: Path\n",
    "#     metric_file_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40a9b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text_summerization_project_nlp.constants import *\n",
    "# from text_summerization_project_nlp.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66be5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConfigurationManager:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         config_filepath = CONFIG_FILE_PATH,\n",
    "#         params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "#         self.config = read_yaml(config_filepath)\n",
    "#         self.params = read_yaml(params_filepath)\n",
    "\n",
    "#         create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "#     def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "#         config = self.config.model_evaluation\n",
    "\n",
    "#         create_directories([config.root_dir])\n",
    "\n",
    "#         model_evaluation_config = ModelEvaluationConfig(\n",
    "#             root_dir=config.root_dir,\n",
    "#             data_path=config.data_path,\n",
    "#             model_path = config.model_path,\n",
    "#             tokenizer_path = config.tokenizer_path,\n",
    "#             metric_file_name = config.metric_file_name\n",
    "           \n",
    "#         )\n",
    "\n",
    "#         return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a28f52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# from datasets import load_dataset, load_from_disk\n",
    "# import evaluate\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df3e25f0-afe4-4710-bc44-f942888d4c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from transformers import pipeline\n",
    "# from datasets import load_from_disk\n",
    "# import evaluate\n",
    "# import nltk\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# class ModelEvaluation:\n",
    "#     def __init__(self, config):\n",
    "#         self.config = config\n",
    "\n",
    "#     def generate_batch_sized_chunks(self, list_of_elements, batch_size):\n",
    "#         for i in range(0, len(list_of_elements), batch_size):\n",
    "#             yield list_of_elements[i:i + batch_size]\n",
    "\n",
    "#     def calculate_metric_on_test_ds(self, dataset, metric, model, tokenizer, batch_size=16, column_text=\"article\", column_summary=\"highlights\"):\n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "#         # Create a summarization pipeline using your trained model\n",
    "#         summarizer = pipeline(\n",
    "#             \"summarization\", \n",
    "#             model=model, \n",
    "#             tokenizer=tokenizer, \n",
    "#             device=device\n",
    "#         )\n",
    "        \n",
    "#         dataset_chunks = list(self.generate_batch_sized_chunks(dataset, batch_size))\n",
    "        \n",
    "#         for dataset_chunk in tqdm(dataset_chunks):\n",
    "#             chunk_text = [d[column_text] for d in dataset_chunk]\n",
    "#             chunk_summary = [d[column_summary] for d in dataset_chunk]\n",
    "            \n",
    "#             # Generate summaries for the current batch\n",
    "#             generated_summaries = summarizer(chunk_text, batch_size=batch_size, min_length=30, max_length=150)\n",
    "            \n",
    "#             # Extract summary text\n",
    "#             generated_summaries = [s['summary_text'] for s in generated_summaries]\n",
    "\n",
    "#             # Add summaries to the metric\n",
    "#             metric.add_batch(\n",
    "#                 predictions=generated_summaries,\n",
    "#                 references=chunk_summary\n",
    "#             )\n",
    "        \n",
    "#         # Compute and return the final scores\n",
    "#         score = metric.compute()\n",
    "#         return score\n",
    "\n",
    "#     def evaluate(self):\n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "#         # Load the model and tokenizer from the saved path\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(self.config.model_path)\n",
    "#         model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n",
    "\n",
    "#         # Load the dataset\n",
    "#         dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "        \n",
    "#         # Load the rouge metric\n",
    "#         rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "#         # Calculate metrics on the test dataset\n",
    "#         score = self.calculate_metric_on_test_ds(\n",
    "#             dataset_samsum_pt['test'], \n",
    "#             rouge, \n",
    "#             model, \n",
    "#             tokenizer, \n",
    "#             batch_size=8,  # Using a batch size of 8 for a smoother run\n",
    "#             column_text='dialogue', \n",
    "#             column_summary='summary'\n",
    "#         )\n",
    "\n",
    "#         # Print the final score\n",
    "#         print(f\"Final ROUGE Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ad6f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-05 15:07:30,189: INFO:common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-09-05 15:07:30,192: INFO:common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-05 15:07:30,192: INFO:common: created directory at: artifacts]\n",
      "[2025-09-05 15:07:30,193: INFO:common: created directory at: artifacts/model_evaluation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-05 15:07:37,185: INFO:rouge_scorer: Using default tokenizer.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from text_summerization_project_nlp.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "# from text_summerization_project_nlp.config.configuration import ConfigurationManager\n",
    "# from text_summerization_project_nlp.components.model_evaluation import ModelEvaluation\n",
    "\n",
    "# try:\n",
    "#     config = ConfigurationManager(\n",
    "#         config_filepath=CONFIG_FILE_PATH,\n",
    "#         params_filepath=PARAMS_FILE_PATH\n",
    "#     )\n",
    "#     model_evaluation_config = config.get_model_evaluation_config()\n",
    "#     model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "#     model_evaluation.evaluate()\n",
    "# except Exception as e:\n",
    "#     raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ae4b326-1a77-49c7-a28f-e68871400cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2cde01e7-83c3-432a-bab8-54b475bca3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"artifacts/model_evaluation/rouge_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88feb12b-0e51-4b51-a1b1-1b794ee9f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rouge1  rouge2    rougeL  rougeLsum\n",
      "0  0.022767     0.0  0.022582   0.022262\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# file_path = \"artifacts/model_evaluation/metrics.csv\"\n",
    "# if os.path.exists(file_path):\n",
    "#     df_rouge_scores = pd.read_csv(file_path)\n",
    "#     print(df_rouge_scores)\n",
    "# else:\n",
    "#     print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d394b-576f-4b2e-b9d9-a3d0dd015416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b95731b-63de-4306-ae80-6a52d5a2b3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\tzip warning: name not matched: End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment\n",
      "\n",
      "zip error: Nothing to do! (try: zip -r summarization_project.zip . -i End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment)\n"
     ]
    }
   ],
   "source": [
    "#!zip -r summarization_project.zip End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f93044bd-930d-4bf7-8f80-df5bd515d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your max_length is set to 150, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "Your max_length is set to 150, but your input_length is only 137. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=68)\n",
      "Your max_length is set to 150, but your input_length is only 129. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=64)\n",
      "Your max_length is set to 150, but your input_length is only 113. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n",
      "Your max_length is set to 150, but your input_length is only 129. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=64)\n",
      "Your max_length is set to 150, but your input_length is only 111. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n",
      "Your max_length is set to 150, but your input_length is only 129. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=64)\n",
      "Your max_length is set to 150, but your input_length is only 104. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n",
      "Your max_length is set to 150, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Your max_length is set to 150, but your input_length is only 30. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Your max_length is set to 150, but your input_length is only 133. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=66)\n",
      "Your max_length is set to 150, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 150, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-05 15:08:06,085: INFO:rouge_scorer: Using default tokenizer.]\n",
      "ROUGE scores for the generated summaries:\n",
      "rouge1: 0.42747345766650413\n",
      "rouge2: 0.20587461236111765\n",
      "rougeL: 0.3305397285489766\n",
      "rougeLsum: 0.3306827834845246\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_from_disk\n",
    "# from transformers import pipeline\n",
    "# from evaluate import load\n",
    "\n",
    "# # Load the dataset dict from the corrected path\n",
    "# dataset = load_from_disk(\"/teamspace/studios/this_studio/End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment/artifacts/data_ingestion/data/samsum_dataset\")\n",
    "\n",
    "# # Select the test split from the dataset dictionary\n",
    "# test_dataset = dataset[\"test\"]\n",
    "\n",
    "# # Create a summarization pipeline using your trained model\n",
    "# # The model path is the directory containing your trained model files\n",
    "# summarizer = pipeline(\"summarization\", model=\"/teamspace/studios/this_studio/End-to-End-Text-Summarization-using-NLP-and-AWS-Deployment/artifacts/model_trainer\")\n",
    "\n",
    "# # Load the ROUGE metric using the new 'evaluate' library\n",
    "# rouge_metric = load(\"rouge\")\n",
    "\n",
    "# # Process the test dataset and generate summaries\n",
    "# generated_summaries = []\n",
    "# reference_summaries = []\n",
    "\n",
    "# # Use a smaller subset of the test data for quick evaluation\n",
    "# num_examples = 20\n",
    "# for i in range(num_examples):\n",
    "#     original_text = test_dataset['dialogue'][i]\n",
    "#     reference_summary = test_dataset['summary'][i]\n",
    "#     generated_summary = summarizer(original_text, max_length=150, min_length=40, truncation=True)[0]['summary_text']\n",
    "    \n",
    "#     generated_summaries.append(generated_summary)\n",
    "#     reference_summaries.append(reference_summary)\n",
    "\n",
    "# # Compute the ROUGE scores\n",
    "# results = rouge_metric.compute(predictions=generated_summaries, references=reference_summaries)\n",
    "\n",
    "# # Print the ROUGE scores\n",
    "# print(\"ROUGE scores for the generated summaries:\")\n",
    "# for key, value in results.items():\n",
    "#     if isinstance(value, dict):\n",
    "#         print(f\"{key}: {value['mid']}\")\n",
    "#     else:\n",
    "#         print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95a3ac4c-f834-4ef3-9534-78691cc31c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'knkarthick/samsum' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded fine-tuned model and tokenizer from './fine-tuned-flan-t5-samsum'\n",
      "[2025-09-05 15:41:09,792: ERROR:load: `trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'knkarthick/samsum' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15602/4003083330.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Evaluation on Validation Set ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='205' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [205/205 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-05 15:42:43,838: INFO:rouge_scorer: Using default tokenizer.]\n",
      "\n",
      "--- Evaluation Results ---\n",
      "eval_loss: nan\n",
      "eval_model_preparation_time: 0.0041\n",
      "eval_rouge1: 0.9216\n",
      "eval_rouge2: 0.0000\n",
      "eval_rougeL: 0.9257\n",
      "eval_rougeLsum: 0.9279\n",
      "eval_runtime: 94.2300\n",
      "eval_samples_per_second: 8.6810\n",
      "eval_steps_per_second: 2.1760\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Acknowledge the use of a GPU for faster training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Load the tokenizer and fine-tuned model from the saved directory ---\n",
    "model_path = \"./fine-tuned-flan-t5-samsum\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "    print(f\"Successfully loaded fine-tuned model and tokenizer from '{model_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from {model_path}: {e}\")\n",
    "    print(\"Please ensure the directory exists and contains all model files.\")\n",
    "    # Exit if the model cannot be loaded\n",
    "    exit()\n",
    "\n",
    "# --- 2. Load and preprocess the SAMSum dataset for evaluation ---\n",
    "dataset = load_dataset(\"knkarthick/samsum\", trust_remote_code=True)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"summarize: {dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=True)\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True, padding=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the validation dataset\n",
    "tokenized_validation_dataset = dataset[\"validation\"].map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "\n",
    "# --- 3. Define the evaluation function ---\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # The model's raw predictions (preds) are logits. We need to convert them to token IDs.\n",
    "    # The OverflowError happens when the tokenizer tries to decode floating-point numbers.\n",
    "    # We fix this by taking the argmax to get the most likely token IDs and then converting to int.\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0] # Take the first element, which should be the logits\n",
    "        \n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    \n",
    "    # Corrected: Explicitly convert predictions to a 64-bit integer type\n",
    "    # This is the most robust way to avoid the OverflowError\n",
    "    preds = preds.astype(np.int64)\n",
    "\n",
    "    # Replace -100 in the labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode the generated predictions and reference labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute the ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Add a friendly output format\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    \n",
    "    return result\n",
    "\n",
    "# --- 4. Set up the Trainer for evaluation ---\n",
    "# IMPORTANT: This is the corrected part. We need to pass a TrainingArguments object.\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./evaluation_output\", # A dummy output directory\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, # Match the training setting\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=\"longest\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args, # Corrected: Pass the training arguments here\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Starting Model Evaluation on Validation Set ---\")\n",
    "evaluation_results = trainer.evaluate(eval_dataset=tokenized_validation_dataset)\n",
    "\n",
    "# --- 5. Print the evaluation results ---\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "for key, value in evaluation_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "print(\"--------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06d4dd-0f96-4af8-9037-216446fe1f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
